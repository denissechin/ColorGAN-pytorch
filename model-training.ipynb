{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as albu\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from utils import print_image, print_train_image, print_test_image\n",
    "from modeling import ResNetUNetGenerator, Discriminator\n",
    "from dataset import Gray_colored_dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available()) else \"cpu\")\n",
    "print(\"device name\", torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = albu.Compose([\n",
    "            albu.SmallestMaxSize(256),\n",
    "            albu.RandomCrop(256, 256), \n",
    "            albu.HorizontalFlip(p=0.2),\n",
    "            albu.VerticalFlip(p=0.2),\n",
    "            albu.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "            ToTensorV2()\n",
    "            ], additional_targets= {'grayscale_image': 'image'})\n",
    "\n",
    "\n",
    "dataset_path = '../input/flickr30k/images'\n",
    "dataset = Gray_colored_dataset(dataset_path, transforms)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "\n",
    "inputs, labels = next(iter(dataloader))\n",
    "print('Input Image')\n",
    "print_image(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Generator and Discriminator\n",
    "***\n",
    "Generator is basically a Unet with some tweaks\n",
    "\n",
    "Discriminator is a typical conv classifier, adjustable for any input image size\n",
    "\n",
    "We will train the model with:\n",
    "- BCELoss from scores of Discriminator\n",
    "- Mean of MAE and RMSE from comparison of generated image and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Colorization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "***\n",
    "Training of this GAN is pretty simple\n",
    "1. Firstly, we update Discriminator's gradients with ground truth image and its error\n",
    "2. Secondly, we generate a colored image and accumulate Discriminator's gradients with processed colored image and its error and update Discriminator's weights\n",
    "3. Then we calculate all the losses' values for Generator and update it's weights\n",
    "\n",
    "Also we freeze resnet layers of Generator for 1/3 of first epoch in order not to wreck well-pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    logger=wandb_logger,    # W&B integration\n",
    "    gpus=-1,                # use all GPU's\n",
    "    max_epochs=15            # number of epochs\n",
    "    )\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check on old photos perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision\n",
    "transform_to_input_image = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(256),\n",
    "            torchvision.transforms.CenterCrop(256),\n",
    "            torchvision.transforms.Grayscale(num_output_channels=3),\n",
    "            torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "gray_test_dataset = ImageFolder('../input/test-images', transform=transform_to_input_image)\n",
    "print_images_from_dataset(model, gray_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': model.generator.state_dict(),\n",
    "            }, './generator.pth')\n",
    "torch.save({\n",
    "            'model_state_dict': model.discriminator.state_dict(),\n",
    "            }, './discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
